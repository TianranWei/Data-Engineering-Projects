## <b>Introduction</b>

A music streaming startup, Sparkify, has grown their user base and song database and want to move their processes and data onto the cloud. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

Task is to build an ETL Pipeline that extracts their data from S3, staging it in Redshift and then transforming data into a set of Dimensional and Fact Tables for their Analytics Team to continue finding Insights to what songs their users are listening to.

## <b>Project Description</b>

Application of Data warehouse and AWS to build an ETL Pipeline for a database hosted on Redshift Will need to load data from S3 to staging tables on Redshift and execute SQL Statements that create fact and dimension tables from these staging tables to create analytics

## <b>Project Datasets</b>

Song Data Path     -->     s3://udacity-dend/song_data

Log Data Path      -->     s3://udacity-dend/log_data

Log Data JSON Path -->     s3://udacity-dend/log_json_path.json

The Sparkify database consists of five tables in the star schema shown below. The fact table is called songplays, and contains a record of each songplay event generated by users of the music streaming app. There are four dimension tables. They store largely normalized data on users, artists, songs and timestamps.

![image](https://user-images.githubusercontent.com/34394130/114559522-1ce36e00-9c6c-11eb-80a2-56707efb1b9f.png)


## How to Run
Prerequisites: Configuration file with login details for an active AWS Redshift cluster and ARN for an IAM role with S3 read access.

1. Run sql_queries.py from terminal or python console to load table create and insert queries
2. Run create_tables.py from terminal or python console to create staging and analytical tables.
3. Run etl.py from terminal or python console to process and load data into data warehouse.
